{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Exploration\n",
    "\n",
    "This notebook provides an initial exploration of the customer churn dataset to understand:\n",
    "- Data structure and schema\n",
    "- Missing values and data quality\n",
    "- Basic statistics and distributions\n",
    "- Event types and user behavior patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Examine Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(file_path):\n",
    "    \"\"\"Load JSON data from file, handling both single JSON objects and JSONL format\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path) as f:\n",
    "            # Try to load as single JSON first\n",
    "            try:\n",
    "                content = json.load(f)\n",
    "                if isinstance(content, list):\n",
    "                    data = content\n",
    "                else:\n",
    "                    data = [content]\n",
    "            except json.JSONDecodeError:\n",
    "                # If that fails, try JSONL format (one JSON per line)\n",
    "                f.seek(0)\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        data.append(json.loads(line))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# Load both datasets\n",
    "print(\"Loading data files...\")\n",
    "mini_data = load_json_data(\"../data/customer_churn_mini.json\")\n",
    "full_data = load_json_data(\"../data/customer_churn.json\")\n",
    "\n",
    "print(f\"Mini dataset: {len(mini_data)} records\")\n",
    "print(f\"Full dataset: {len(full_data)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrames\n",
    "df_mini = pd.DataFrame(mini_data)\n",
    "df_full = pd.DataFrame(full_data)\n",
    "\n",
    "print(\"Mini Dataset Shape:\", df_mini.shape)\n",
    "print(\"Full Dataset Shape:\", df_full.shape)\n",
    "\n",
    "# Use the mini dataset for initial exploration\n",
    "df = df_mini.copy()\n",
    "print(f\"\\nUsing mini dataset for exploration: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Schema and Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data structure\n",
    "print(\"=== DATA SCHEMA ===\\n\")\n",
    "print(\"Column names and data types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first few records\n",
    "print(\"=== FIRST 3 RECORDS ===\\n\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"Record {i+1}:\")\n",
    "    print(df.iloc[i].to_dict())\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine unique values for key categorical columns\n",
    "categorical_cols = [\"page\", \"auth\", \"method\", \"level\", \"gender\"]\n",
    "\n",
    "print(\"=== CATEGORICAL COLUMNS ANALYSIS ===\\n\")\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col.upper()}:\")\n",
    "        print(f\"  Unique values: {df[col].nunique()}\")\n",
    "\n",
    "        # For EDA, we want to see NaN patterns, not hide them\n",
    "        # Count NaN values separately to understand data quality\n",
    "        nan_count = df[col].isnull().sum()\n",
    "        total_count = len(df)\n",
    "        nan_percentage = (nan_count / total_count) * 100\n",
    "\n",
    "        if nan_count > 0:\n",
    "            print(f\"  Missing values: {nan_count} ({nan_percentage:.2f}%)\")\n",
    "\n",
    "        # Get unique values excluding NaN for sorting (to avoid TypeError)\n",
    "        # But we'll report the missing values separately above\n",
    "        unique_vals_no_nan = df[col].dropna().unique()\n",
    "\n",
    "        try:\n",
    "            sorted_vals = sorted(unique_vals_no_nan)\n",
    "        except TypeError:\n",
    "            # Fallback: convert to string if mixed types still cause issues\n",
    "            sorted_vals = sorted([str(x) for x in unique_vals_no_nan])\n",
    "\n",
    "        # Show non-missing unique values\n",
    "        print(f\"  Non-missing values: {sorted_vals}\")\n",
    "\n",
    "        # Value counts include NaN by default, which is what we want for EDA\n",
    "        # This shows us the complete picture including missing data patterns\n",
    "        print(\"  Value counts (including missing):\")\n",
    "        value_counts = df[col].value_counts(dropna=False).head(10)\n",
    "        print(value_counts)\n",
    "\n",
    "        # Add interpretation comments for different column types\n",
    "        if col == \"page\":\n",
    "            if nan_count > 0:\n",
    "                print(\n",
    "                    \"     WARNING: 'page' should never be missing - indicates data quality issues\"\n",
    "                )\n",
    "        elif col == \"level\":\n",
    "            if nan_count > 0:\n",
    "                print(\n",
    "                    \"     INFO: Missing subscription level might indicate guest users or data capture gaps\"\n",
    "                )\n",
    "        elif col == \"gender\":\n",
    "            if nan_count > 0:\n",
    "                print(\n",
    "                    \"     INFO: Missing gender is common - users may not provide demographic info\"\n",
    "                )\n",
    "        elif col in [\"auth\", \"method\"]:\n",
    "            if nan_count > 0:\n",
    "                print(\n",
    "                    f\"     WARNING: '{col}' should typically be present for all requests\"\n",
    "                )\n",
    "\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Values and Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis with visualization\n",
    "print(\"=== MISSING VALUES ANALYSIS ===\\n\")\n",
    "missing_info = pd.DataFrame(\n",
    "    {\n",
    "        \"Column\": df.columns,\n",
    "        \"Missing_Count\": df.isnull().sum(),\n",
    "        \"Missing_Percentage\": (df.isnull().sum() / len(df)) * 100,\n",
    "        \"Data_Type\": df.dtypes,\n",
    "    }\n",
    ")\n",
    "\n",
    "missing_info = missing_info.sort_values(\"Missing_Percentage\", ascending=False)\n",
    "print(missing_info)\n",
    "\n",
    "# Highlight columns with significant missing values\n",
    "high_missing = missing_info[missing_info[\"Missing_Percentage\"] > 10]\n",
    "if not high_missing.empty:\n",
    "    print(\"\\n   Columns with >10% missing values:\")\n",
    "    print(high_missing[[\"Column\", \"Missing_Percentage\"]])\n",
    "\n",
    "# Visualize missing values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Missing values heatmap\n",
    "missing_cols = missing_info[missing_info[\"Missing_Count\"] > 0][\"Column\"].head(10)\n",
    "if len(missing_cols) > 0:\n",
    "    missing_matrix = df[missing_cols].isnull()\n",
    "    sns.heatmap(\n",
    "        missing_matrix,\n",
    "        cbar=True,\n",
    "        ax=axes[0],\n",
    "        yticklabels=False,\n",
    "        cmap=\"viridis\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    axes[0].set_title(\"Missing Values Pattern (Sample of Data)\")\n",
    "    axes[0].set_xlabel(\"Columns with Missing Values\")\n",
    "else:\n",
    "    axes[0].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        \"No Missing Values Found\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        transform=axes[0].transAxes,\n",
    "        fontsize=14,\n",
    "    )\n",
    "    axes[0].set_title(\"Missing Values Pattern\")\n",
    "\n",
    "# Missing values bar chart\n",
    "missing_counts = missing_info[missing_info[\"Missing_Count\"] > 0].sort_values(\n",
    "    \"Missing_Percentage\", ascending=True\n",
    ")\n",
    "if not missing_counts.empty:\n",
    "    missing_counts.plot(\n",
    "        x=\"Column\",\n",
    "        y=\"Missing_Percentage\",\n",
    "        kind=\"barh\",\n",
    "        ax=axes[1],\n",
    "        color=\"salmon\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    axes[1].set_title(\"Missing Values Percentage by Column\")\n",
    "    axes[1].set_xlabel(\"Missing Percentage (%)\")\n",
    "    axes[1].set_ylabel(\"Columns\")\n",
    "else:\n",
    "    axes[1].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        \"No Missing Values Found\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        transform=axes[1].transAxes,\n",
    "        fontsize=14,\n",
    "    )\n",
    "    axes[1].set_title(\"Missing Values Percentage\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Missing value patterns analysis\n",
    "print(\"\\n=== MISSING VALUE PATTERNS ===\")\n",
    "if len(missing_cols) > 1:\n",
    "    # Check if missing values occur together\n",
    "    missing_patterns = df[missing_cols].isnull().value_counts()\n",
    "    print(\"Top missing value patterns:\")\n",
    "    print(missing_patterns.head())\n",
    "else:\n",
    "    print(\"Insufficient columns with missing values for pattern analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "print(\"=== DATA QUALITY CHECKS ===\\n\")\n",
    "\n",
    "# Check for duplicate records\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate records: {duplicates} ({duplicates/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Check timestamp format and range\n",
    "if \"ts\" in df.columns:\n",
    "    print(\"\\nTimestamp analysis:\")\n",
    "    print(f\"  Min timestamp: {df['ts'].min()}\")\n",
    "    print(f\"  Max timestamp: {df['ts'].max()}\")\n",
    "\n",
    "    # Convert to datetime for better understanding\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\")\n",
    "    print(f\"  Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "    print(f\"  Time span: {(df['datetime'].max() - df['datetime'].min()).days} days\")\n",
    "\n",
    "# Check user ID consistency\n",
    "if \"userId\" in df.columns:\n",
    "    print(\"\\nUser ID analysis:\")\n",
    "    print(f\"  Unique users: {df['userId'].nunique()}\")\n",
    "    print(f\"  Total events: {len(df)}\")\n",
    "    print(f\"  Avg events per user: {len(df) / df['userId'].nunique():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Statistics and Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns analysis\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"=== NUMERICAL COLUMNS STATISTICS ===\\n\")\n",
    "print(f\"Numerical columns: {numerical_cols}\")\n",
    "\n",
    "if numerical_cols:\n",
    "    print(\"\\nDescriptive statistics:\")\n",
    "    print(df[numerical_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of key numerical columns with better formatting\n",
    "if numerical_cols:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Focus on key columns for better visualization\n",
    "    key_cols = [\"itemInSession\", \"length\", \"sessionId\", \"status\"]\n",
    "\n",
    "    for i, col in enumerate(key_cols):\n",
    "        if i < len(axes) and col in df.columns:\n",
    "            # Handle missing values by dropping them for visualization\n",
    "            data_clean = df[col].dropna()\n",
    "\n",
    "            if col == \"status\":\n",
    "                # Status codes as categorical\n",
    "                status_counts = data_clean.value_counts()\n",
    "                status_counts.plot(kind=\"bar\", ax=axes[i], alpha=0.7, color=\"skyblue\")\n",
    "                axes[i].set_title(f\"Distribution of {col}\")\n",
    "                axes[i].set_ylabel(\"Count\")\n",
    "                axes[i].tick_params(axis=\"x\", rotation=45)\n",
    "            else:\n",
    "                # Regular histogram for continuous variables\n",
    "                data_clean.hist(\n",
    "                    bins=50,\n",
    "                    ax=axes[i],\n",
    "                    alpha=0.7,\n",
    "                    color=\"lightgreen\",\n",
    "                    edgecolor=\"black\",\n",
    "                )\n",
    "                axes[i].set_title(f\"Distribution of {col}\")\n",
    "                axes[i].set_xlabel(col)\n",
    "                axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "                # Add mean line\n",
    "                mean_val = data_clean.mean()\n",
    "                axes[i].axvline(\n",
    "                    mean_val,\n",
    "                    color=\"red\",\n",
    "                    linestyle=\"--\",\n",
    "                    alpha=0.7,\n",
    "                    label=f\"Mean: {mean_val:.1f}\",\n",
    "                )\n",
    "                axes[i].legend()\n",
    "\n",
    "    # User activity distribution\n",
    "    if \"userId\" in df.columns:\n",
    "        user_events = df.groupby(\"userId\").size()\n",
    "        user_events.hist(\n",
    "            bins=30, ax=axes[4], alpha=0.7, color=\"orange\", edgecolor=\"black\"\n",
    "        )\n",
    "        axes[4].set_title(\"Events per User Distribution\")\n",
    "        axes[4].set_xlabel(\"Number of Events\")\n",
    "        axes[4].set_ylabel(\"Number of Users\")\n",
    "\n",
    "    # Song length analysis (excluding missing)\n",
    "    if \"length\" in df.columns:\n",
    "        length_clean = df[\"length\"].dropna()\n",
    "        if len(length_clean) > 0:\n",
    "            # Convert to minutes for better interpretation\n",
    "            length_minutes = length_clean / 60\n",
    "            length_minutes.hist(\n",
    "                bins=50, ax=axes[5], alpha=0.7, color=\"purple\", edgecolor=\"black\"\n",
    "            )\n",
    "            axes[5].set_title(\"Song Length Distribution\")\n",
    "            axes[5].set_xlabel(\"Song Length (minutes)\")\n",
    "            axes[5].set_ylabel(\"Frequency\")\n",
    "            axes[5].axvline(\n",
    "                length_minutes.mean(),\n",
    "                color=\"red\",\n",
    "                linestyle=\"--\",\n",
    "                alpha=0.7,\n",
    "                label=f\"Mean: {length_minutes.mean():.1f} min\",\n",
    "            )\n",
    "            axes[5].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Event Types and User Behavior Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event types analysis with improved visualizations\n",
    "print(\"=== EVENT TYPES ANALYSIS ===\\n\")\n",
    "\n",
    "if \"page\" in df.columns:\n",
    "    event_counts = df[\"page\"].value_counts()\n",
    "    print(\"Event type distribution:\")\n",
    "    print(event_counts)\n",
    "\n",
    "    # Create subplots for better visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "    # Top 10 events bar chart\n",
    "    top_events = event_counts.head(10)\n",
    "    top_events.plot(kind=\"bar\", ax=axes[0, 0], color=\"steelblue\", alpha=0.8)\n",
    "    axes[0, 0].set_title(\"Top 10 Event Types Distribution\")\n",
    "    axes[0, 0].set_xlabel(\"Event Type\")\n",
    "    axes[0, 0].set_ylabel(\"Count\")\n",
    "    axes[0, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Pie chart for TOP 5 events only (to avoid overcrowding)\n",
    "    top_5_events = event_counts.head(5)\n",
    "    other_events_count = event_counts.iloc[5:].sum()\n",
    "\n",
    "    # Create data for pie chart with \"Others\" category\n",
    "    pie_data = top_5_events.copy()\n",
    "    pie_data[\"Others\"] = other_events_count\n",
    "\n",
    "    # Create pie chart with better spacing and readable labels\n",
    "    wedges, texts, autotexts = axes[0, 1].pie(\n",
    "        pie_data.values,\n",
    "        labels=pie_data.index,\n",
    "        autopct=\"%1.1f%%\",\n",
    "        startangle=90,\n",
    "        colors=sns.color_palette(\"Set2\", len(pie_data)),\n",
    "        textprops={\"fontsize\": 10},\n",
    "    )\n",
    "\n",
    "    # Position labels outside the pie to avoid overlap\n",
    "    for text in texts:\n",
    "        text.set_fontsize(9)\n",
    "\n",
    "    # Move percentage labels further out\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color(\"white\")\n",
    "        autotext.set_fontweight(\"bold\")\n",
    "        autotext.set_fontsize(9)\n",
    "\n",
    "    axes[0, 1].set_title(\n",
    "        \"Top 5 Events Distribution\\n(Others = Remaining 17 Events Combined)\",\n",
    "        fontsize=11,\n",
    "        pad=15,\n",
    "    )\n",
    "\n",
    "    # All events horizontal bar chart (sorted)\n",
    "    event_counts.plot(kind=\"barh\", ax=axes[1, 0], color=\"lightcoral\", alpha=0.8)\n",
    "    axes[1, 0].set_title(\"All Event Types (Complete List)\")\n",
    "    axes[1, 0].set_xlabel(\"Count\")\n",
    "    axes[1, 0].set_ylabel(\"Event Type\")\n",
    "\n",
    "    # Log scale for better visualization of small events\n",
    "    event_counts.plot(\n",
    "        kind=\"bar\", ax=axes[1, 1], color=\"darkgreen\", alpha=0.8, logy=True\n",
    "    )\n",
    "    axes[1, 1].set_title(\"Event Types Distribution (Log Scale)\")\n",
    "    axes[1, 1].set_xlabel(\"Event Type\")\n",
    "    axes[1, 1].set_ylabel(\"Count (Log Scale)\")\n",
    "    axes[1, 1].tick_params(axis=\"x\", rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate percentages\n",
    "    event_pct = (event_counts / event_counts.sum()) * 100\n",
    "    print(\"\\nEvent type percentages:\")\n",
    "    for event, pct in event_pct.head(10).items():\n",
    "        print(f\"  {event}: {pct:.2f}%\")\n",
    "\n",
    "    # Event categories analysis\n",
    "    print(\"\\n=== EVENT CATEGORIZATION ===\")\n",
    "    music_events = [\"NextSong\", \"Thumbs Up\", \"Thumbs Down\", \"Add to Playlist\"]\n",
    "    navigation_events = [\"Home\", \"About\", \"Help\", \"Settings\", \"Save Settings\"]\n",
    "    auth_events = [\"Login\", \"Logout\", \"Register\", \"Submit Registration\"]\n",
    "    subscription_events = [\n",
    "        \"Upgrade\",\n",
    "        \"Submit Upgrade\",\n",
    "        \"Downgrade\",\n",
    "        \"Submit Downgrade\",\n",
    "        \"Cancel\",\n",
    "        \"Cancellation Confirmation\",\n",
    "    ]\n",
    "    social_events = [\"Add Friend\"]\n",
    "    other_events = [\"Roll Advert\", \"Error\"]\n",
    "\n",
    "    categories = {\n",
    "        \"Music Interaction\": music_events,\n",
    "        \"Navigation\": navigation_events,\n",
    "        \"Authentication\": auth_events,\n",
    "        \"Subscription\": subscription_events,\n",
    "        \"Social\": social_events,\n",
    "        \"Other\": other_events,\n",
    "    }\n",
    "\n",
    "    category_counts = {}\n",
    "    for category, events in categories.items():\n",
    "        count = event_counts[event_counts.index.isin(events)].sum()\n",
    "        category_counts[category] = count\n",
    "        print(f\"  {category}: {count:,} events ({count/event_counts.sum()*100:.1f}%)\")\n",
    "\n",
    "    # Visualize event categories with clean layout\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Bar chart for event categories (more readable than pie for 6 categories)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    category_series = pd.Series(category_counts)\n",
    "    bars = category_series.plot(\n",
    "        kind=\"bar\", color=sns.color_palette(\"Set2\", len(category_counts)), alpha=0.8\n",
    "    )\n",
    "    plt.title(\"Event Categories Distribution\", fontsize=14)\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Number of Events\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars.patches):\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + height * 0.01,\n",
    "            f\"{int(height):,}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    # Pie chart for subscription level breakdown (more meaningful)\n",
    "    if \"level\" in df.columns:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        level_counts = df[\"level\"].value_counts(dropna=True)\n",
    "        wedges, texts, autotexts = plt.pie(\n",
    "            level_counts.values,\n",
    "            labels=level_counts.index,\n",
    "            autopct=\"%1.1f%%\",\n",
    "            startangle=90,\n",
    "            colors=[\"#ff9999\", \"#66b3ff\"],\n",
    "            textprops={\"fontsize\": 12},\n",
    "        )\n",
    "\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color(\"white\")\n",
    "            autotext.set_fontweight(\"bold\")\n",
    "            autotext.set_fontsize(11)\n",
    "\n",
    "        plt.title(\"User Subscription Distribution\", fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User activity patterns\n",
    "print(\"=== USER ACTIVITY PATTERNS ===\\n\")\n",
    "\n",
    "if \"userId\" in df.columns:\n",
    "    user_activity = (\n",
    "        df.groupby(\"userId\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"page\": \"count\",  # Total events per user\n",
    "                \"sessionId\": \"nunique\",  # Unique sessions per user\n",
    "                \"ts\": [\"min\", \"max\"],  # Activity time range\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    user_activity.columns = [\n",
    "        \"userId\",\n",
    "        \"total_events\",\n",
    "        \"unique_sessions\",\n",
    "        \"first_event\",\n",
    "        \"last_event\",\n",
    "    ]\n",
    "\n",
    "    # Calculate activity duration in days\n",
    "    user_activity[\"activity_duration_days\"] = (\n",
    "        user_activity[\"last_event\"] - user_activity[\"first_event\"]\n",
    "    ) / (1000 * 60 * 60 * 24)\n",
    "    user_activity[\"events_per_session\"] = (\n",
    "        user_activity[\"total_events\"] / user_activity[\"unique_sessions\"]\n",
    "    )\n",
    "\n",
    "    print(\"User activity statistics:\")\n",
    "    print(\n",
    "        user_activity[\n",
    "            [\n",
    "                \"total_events\",\n",
    "                \"unique_sessions\",\n",
    "                \"activity_duration_days\",\n",
    "                \"events_per_session\",\n",
    "            ]\n",
    "        ].describe()\n",
    "    )\n",
    "\n",
    "    # Visualize user activity distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    user_activity[\"total_events\"].hist(bins=30, ax=axes[0, 0], alpha=0.7)\n",
    "    axes[0, 0].set_title(\"Distribution of Total Events per User\")\n",
    "    axes[0, 0].set_xlabel(\"Total Events\")\n",
    "\n",
    "    user_activity[\"unique_sessions\"].hist(bins=30, ax=axes[0, 1], alpha=0.7)\n",
    "    axes[0, 1].set_title(\"Distribution of Sessions per User\")\n",
    "    axes[0, 1].set_xlabel(\"Unique Sessions\")\n",
    "\n",
    "    user_activity[\"activity_duration_days\"].hist(bins=30, ax=axes[1, 0], alpha=0.7)\n",
    "    axes[1, 0].set_title(\"Distribution of Activity Duration (Days)\")\n",
    "    axes[1, 0].set_xlabel(\"Days\")\n",
    "\n",
    "    user_activity[\"events_per_session\"].hist(bins=30, ax=axes[1, 1], alpha=0.7)\n",
    "    axes[1, 1].set_title(\"Distribution of Events per Session\")\n",
    "    axes[1, 1].set_xlabel(\"Events per Session\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscription level analysis - REMOVED REDUNDANT PIE CHART\n",
    "if \"level\" in df.columns:\n",
    "    print(\"=== SUBSCRIPTION LEVEL ANALYSIS ===\\n\")\n",
    "\n",
    "    # Get value counts excluding NaN for cleaner analysis\n",
    "    level_dist = df[\"level\"].value_counts(\n",
    "        dropna=False\n",
    "    )  # Include NaN to see missing data\n",
    "    print(\"Subscription level distribution:\")\n",
    "    print(level_dist)\n",
    "\n",
    "    # Check for missing values and report\n",
    "    missing_level_count = df[\"level\"].isnull().sum()\n",
    "    if missing_level_count > 0:\n",
    "        print(\n",
    "            f\"\\n  Note: {missing_level_count} records have missing subscription level\"\n",
    "        )\n",
    "\n",
    "    # Events by subscription level (exclude NaN for cleaner pivot)\n",
    "    df_no_missing_level = df.dropna(subset=[\"level\"])  # Only for this analysis\n",
    "    level_events = (\n",
    "        df_no_missing_level.groupby([\"level\", \"page\"]).size().unstack(fill_value=0)\n",
    "    )\n",
    "\n",
    "    print(\"\\nDIAGNOSTIC: Full event breakdown by subscription level:\")\n",
    "    print(f\"Total event types in data: {level_events.shape[1]}\")\n",
    "    print(\n",
    "        f\"Event types that appear for both paid and free users: {(level_events > 0).all().sum()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Event types that appear for at least one subscription level: {(level_events > 0).any().sum()}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nTop events by subscription level (showing top 10 overall):\")\n",
    "    top_events_overall = level_events.sum().nlargest(10)\n",
    "    print(\"Overall event ranking:\")\n",
    "    print(top_events_overall)\n",
    "\n",
    "    print(\"\\nTop 10 events broken down by subscription level:\")\n",
    "    top_10_events_df = level_events.loc[:, top_events_overall.index]\n",
    "    print(top_10_events_df)\n",
    "\n",
    "    # Single focused visualization - Events by subscription level comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Bar chart - Show top 5 events that have activity in both subscription levels\n",
    "    # Filter to events that have non-zero counts for both paid and free\n",
    "    events_with_both_levels = level_events.loc[:, (level_events > 0).all()]\n",
    "\n",
    "    if not events_with_both_levels.empty and events_with_both_levels.shape[1] >= 3:\n",
    "        # Take top 5 events that appear in both levels\n",
    "        top_events_both_levels = events_with_both_levels.sum().nlargest(5)\n",
    "        events_to_plot = level_events.loc[:, top_events_both_levels.index]\n",
    "\n",
    "        events_to_plot.plot(kind=\"bar\", ax=axes[0])\n",
    "        axes[0].set_title(\n",
    "            f\"Top {len(top_events_both_levels)} Events by Subscription Level\\n(Events present in both paid & free)\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "        axes[0].set_xlabel(\"Subscription Level\")\n",
    "        axes[0].set_ylabel(\"Event Count\")\n",
    "        axes[0].tick_params(\n",
    "            axis=\"x\", rotation=0\n",
    "        )  # Keep subscription level labels horizontal\n",
    "        axes[0].legend(title=\"Event Type\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        print(\"\\nCHART EXPLANATION:\")\n",
    "        print(\n",
    "            f\"Showing {len(top_events_both_levels)} event types that have activity in BOTH paid and free users\"\n",
    "        )\n",
    "        print(f\"These are: {list(top_events_both_levels.index)}\")\n",
    "\n",
    "    else:\n",
    "        # Fallback: show top events even if they don't appear in both levels\n",
    "        if not level_events.empty:\n",
    "            available_events = min(level_events.shape[1], 5)\n",
    "            top_available = level_events.sum().nlargest(available_events)\n",
    "            level_events.loc[:, top_available.index].plot(kind=\"bar\", ax=axes[0])\n",
    "            axes[0].set_title(\n",
    "                f\"Top {available_events} Events by Subscription Level\\n(May not appear in both levels)\",\n",
    "                fontsize=12,\n",
    "            )\n",
    "            axes[0].set_xlabel(\"Subscription Level\")\n",
    "            axes[0].set_ylabel(\"Event Count\")\n",
    "            axes[0].tick_params(axis=\"x\", rotation=0)\n",
    "            axes[0].legend(\n",
    "                title=\"Event Type\", bbox_to_anchor=(1.05, 1), loc=\"upper left\"\n",
    "            )\n",
    "            axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "            print(\"\\nðŸ“Š CHART EXPLANATION:\")\n",
    "            print(\n",
    "                f\"Showing top {available_events} events overall (some may not appear in both subscription levels)\"\n",
    "            )\n",
    "        else:\n",
    "            axes[0].text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                \"Insufficient data for event analysis\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=axes[0].transAxes,\n",
    "            )\n",
    "            axes[0].set_title(\"Events by Subscription Level\")\n",
    "\n",
    "    # Subscription behavior comparison - Show event diversity by level\n",
    "    if not level_events.empty:\n",
    "        # Calculate event diversity (number of different event types) per subscription level\n",
    "        event_diversity = (level_events > 0).sum(axis=1)\n",
    "\n",
    "        ax = axes[1]\n",
    "        bars = event_diversity.plot(\n",
    "            kind=\"bar\", ax=ax, color=[\"#ff9999\", \"#66b3ff\"], alpha=0.8\n",
    "        )\n",
    "        ax.set_title(\"Event Type Diversity by Subscription Level\", fontsize=12)\n",
    "        ax.set_xlabel(\"Subscription Level\")\n",
    "        ax.set_ylabel(\"Number of Different Event Types Used\")\n",
    "        ax.tick_params(axis=\"x\", rotation=0)\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for bar in bars.patches:\n",
    "            height = bar.get_height()\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height + height * 0.01,\n",
    "                f\"{int(height)}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=11,\n",
    "            )\n",
    "\n",
    "        print(\"\\nEVENT DIVERSITY ANALYSIS:\")\n",
    "        for level, diversity in event_diversity.items():\n",
    "            total_events = level_events.loc[level].sum()\n",
    "            print(\n",
    "                f\"  {level.upper()} users: Use {diversity} different event types (Total: {total_events:,} events)\"\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Additional analysis: Show events that are unique to each subscription level\n",
    "    if not level_events.empty:\n",
    "        print(\"\\nADDITIONAL INSIGHTS:\")\n",
    "\n",
    "        # Events exclusive to paid users\n",
    "        paid_only_events = level_events.loc[\"paid\"][level_events.loc[\"paid\"] > 0]\n",
    "        free_only_events = level_events.loc[\"free\"][level_events.loc[\"free\"] > 0]\n",
    "\n",
    "        paid_exclusive = paid_only_events[level_events.loc[\"free\"] == 0]\n",
    "        free_exclusive = free_only_events[level_events.loc[\"paid\"] == 0]\n",
    "\n",
    "        if not paid_exclusive.empty:\n",
    "            print(f\"\\nEvents exclusive to PAID users: {list(paid_exclusive.index)}\")\n",
    "            print(paid_exclusive.head())\n",
    "        else:\n",
    "            print(\"\\nNo events are exclusive to PAID users\")\n",
    "\n",
    "        if not free_exclusive.empty:\n",
    "            print(f\"\\nEvents exclusive to FREE users: {list(free_exclusive.index)}\")\n",
    "            print(free_exclusive.head())\n",
    "        else:\n",
    "            print(\"\\nNo events are exclusive to FREE users\")\n",
    "\n",
    "        # Event distribution percentages by level\n",
    "        print(\"\\nEvent distribution percentages by subscription level:\")\n",
    "        level_event_pct = level_events.div(level_events.sum(axis=1), axis=0) * 100\n",
    "        print(level_event_pct.loc[:, top_events_overall.head().index].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temporal Patterns Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced temporal patterns analysis\n",
    "if \"datetime\" in df.columns:\n",
    "    print(\"=== TEMPORAL PATTERNS ===\\n\")\n",
    "\n",
    "    # Add temporal features\n",
    "    df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "    df[\"day_of_week\"] = df[\"datetime\"].dt.dayofweek\n",
    "    df[\"date\"] = df[\"datetime\"].dt.date\n",
    "\n",
    "    # Hourly activity pattern\n",
    "    hourly_activity = df[\"hour\"].value_counts().sort_index()\n",
    "    print(\"Activity by hour of day:\")\n",
    "    print(hourly_activity)\n",
    "\n",
    "    # Daily activity pattern\n",
    "    daily_activity = df[\"day_of_week\"].value_counts().sort_index()\n",
    "    day_names = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "    print(\"\\nActivity by day of week:\")\n",
    "    for day_num, count in daily_activity.items():\n",
    "        print(f\"  {day_names[day_num]}: {count}\")\n",
    "\n",
    "    # Enhanced temporal visualizations\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "\n",
    "    # Hourly pattern with style improvements\n",
    "    hourly_activity.plot(\n",
    "        kind=\"bar\", ax=axes[0, 0], color=\"skyblue\", alpha=0.8, edgecolor=\"black\"\n",
    "    )\n",
    "    axes[0, 0].set_title(\n",
    "        \"Activity Distribution by Hour of Day\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[0, 0].set_xlabel(\"Hour of Day\")\n",
    "    axes[0, 0].set_ylabel(\"Number of Events\")\n",
    "    axes[0, 0].tick_params(axis=\"x\", rotation=0)\n",
    "    axes[0, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Daily pattern with day names\n",
    "    daily_activity_named = pd.Series(\n",
    "        daily_activity.values, index=[day_names[i] for i in daily_activity.index]\n",
    "    )\n",
    "    daily_activity_named.plot(\n",
    "        kind=\"bar\", ax=axes[0, 1], color=\"lightgreen\", alpha=0.8, edgecolor=\"black\"\n",
    "    )\n",
    "    axes[0, 1].set_title(\n",
    "        \"Activity Distribution by Day of Week\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[0, 1].set_xlabel(\"Day of Week\")\n",
    "    axes[0, 1].set_ylabel(\"Number of Events\")\n",
    "    axes[0, 1].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[0, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Activity timeline with smoothing\n",
    "    daily_timeline = df.groupby(\"date\").size()\n",
    "    daily_timeline.index = pd.to_datetime(daily_timeline.index)\n",
    "    daily_timeline.plot(ax=axes[1, 0], color=\"orange\", alpha=0.8, linewidth=2)\n",
    "    axes[1, 0].set_title(\"Daily Activity Timeline\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[1, 0].set_xlabel(\"Date\")\n",
    "    axes[1, 0].set_ylabel(\"Daily Event Count\")\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Add 7-day rolling average\n",
    "    rolling_avg = daily_timeline.rolling(window=7, center=True).mean()\n",
    "    rolling_avg.plot(\n",
    "        ax=axes[1, 0],\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        linewidth=2,\n",
    "        label=\"7-day Moving Average\",\n",
    "    )\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    # Session length distribution with better binning\n",
    "    if \"sessionId\" in df.columns:\n",
    "        session_lengths = df.groupby([\"userId\", \"sessionId\"]).size()\n",
    "        session_lengths.hist(\n",
    "            bins=50, ax=axes[1, 1], alpha=0.7, color=\"purple\", edgecolor=\"black\"\n",
    "        )\n",
    "        axes[1, 1].set_title(\n",
    "            \"Session Length Distribution\", fontsize=14, fontweight=\"bold\"\n",
    "        )\n",
    "        axes[1, 1].set_xlabel(\"Events per Session\")\n",
    "        axes[1, 1].set_ylabel(\"Number of Sessions\")\n",
    "        axes[1, 1].axvline(\n",
    "            session_lengths.mean(),\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            label=f\"Mean: {session_lengths.mean():.1f}\",\n",
    "        )\n",
    "        axes[1, 1].axvline(\n",
    "            session_lengths.median(),\n",
    "            color=\"green\",\n",
    "            linestyle=\"--\",\n",
    "            label=f\"Median: {session_lengths.median():.1f}\",\n",
    "        )\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Heatmap of activity by hour and day\n",
    "    if len(df) > 1000:  # Only if we have enough data\n",
    "        hour_day_activity = (\n",
    "            df.groupby([\"day_of_week\", \"hour\"]).size().unstack(fill_value=0)\n",
    "        )\n",
    "        hour_day_activity.index = [day_names[i] for i in hour_day_activity.index]\n",
    "\n",
    "        sns.heatmap(\n",
    "            hour_day_activity,\n",
    "            ax=axes[2, 0],\n",
    "            cmap=\"YlOrRd\",\n",
    "            cbar_kws={\"label\": \"Number of Events\"},\n",
    "            annot=False,\n",
    "        )\n",
    "        axes[2, 0].set_title(\n",
    "            \"Activity Heatmap: Hour vs Day of Week\", fontsize=14, fontweight=\"bold\"\n",
    "        )\n",
    "        axes[2, 0].set_xlabel(\"Hour of Day\")\n",
    "        axes[2, 0].set_ylabel(\"Day of Week\")\n",
    "\n",
    "    # User activity over time (sample of most active users)\n",
    "    if \"userId\" in df.columns:\n",
    "        top_users = df[\"userId\"].value_counts().head(5).index\n",
    "        user_timeline = (\n",
    "            df[df[\"userId\"].isin(top_users)]\n",
    "            .groupby([\"date\", \"userId\"])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "        )\n",
    "        user_timeline.index = pd.to_datetime(user_timeline.index)\n",
    "\n",
    "        for user in top_users:\n",
    "            if user in user_timeline.columns:\n",
    "                user_timeline[user].plot(\n",
    "                    ax=axes[2, 1], alpha=0.7, linewidth=2, label=f\"User {user}\"\n",
    "                )\n",
    "\n",
    "        axes[2, 1].set_title(\n",
    "            \"Top 5 Users Activity Timeline\", fontsize=14, fontweight=\"bold\"\n",
    "        )\n",
    "        axes[2, 1].set_xlabel(\"Date\")\n",
    "        axes[2, 1].set_ylabel(\"Daily Events\")\n",
    "        axes[2, 1].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        axes[2, 1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Statistical summary of temporal patterns\n",
    "    print(\"\\n=== TEMPORAL INSIGHTS ===\")\n",
    "    peak_hour = hourly_activity.idxmax()\n",
    "    peak_day = day_names[daily_activity.idxmax()]\n",
    "    print(f\"Peak activity hour: {peak_hour}:00\")\n",
    "    print(f\"Peak activity day: {peak_day}\")\n",
    "    print(f\"Average daily events: {daily_timeline.mean():.1f}\")\n",
    "    print(\n",
    "        f\"Most active date: {daily_timeline.idxmax()} ({daily_timeline.max()} events)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Least active date: {daily_timeline.idxmin()} ({daily_timeline.min()} events)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Potential Churn Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis of numerical features\n",
    "print(\"=== CORRELATION ANALYSIS ===\\n\")\n",
    "\n",
    "# Select numerical columns for correlation analysis\n",
    "numerical_cols_for_corr = [\n",
    "    \"ts\",\n",
    "    \"sessionId\",\n",
    "    \"status\",\n",
    "    \"itemInSession\",\n",
    "    \"registration\",\n",
    "    \"length\",\n",
    "]\n",
    "existing_num_cols = [col for col in numerical_cols_for_corr if col in df.columns]\n",
    "\n",
    "if len(existing_num_cols) > 1:\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df[existing_num_cols].corr()\n",
    "\n",
    "    print(\"Correlation Matrix:\")\n",
    "    print(correlation_matrix.round(3))\n",
    "\n",
    "    # Visualize correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Create heatmap\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle\n",
    "    sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        mask=mask,\n",
    "        annot=True,\n",
    "        cmap=\"RdYlBu_r\",\n",
    "        center=0,\n",
    "        square=True,\n",
    "        fmt=\".2f\",\n",
    "        cbar_kws={\"label\": \"Correlation Coefficient\"},\n",
    "    )\n",
    "    plt.title(\n",
    "        \"Correlation Matrix of Numerical Features\", fontsize=16, fontweight=\"bold\"\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Identify strong correlations\n",
    "    print(\"\\n=== STRONG CORRELATIONS (>0.5 or <-0.5) ===\")\n",
    "    strong_corr_pairs = []\n",
    "\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.5:\n",
    "                strong_corr_pairs.append(\n",
    "                    (\n",
    "                        correlation_matrix.columns[i],\n",
    "                        correlation_matrix.columns[j],\n",
    "                        corr_val,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    if strong_corr_pairs:\n",
    "        for col1, col2, corr in strong_corr_pairs:\n",
    "            print(f\"  {col1} <-> {col2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"  No strong correlations found between numerical features\")\n",
    "\n",
    "# Analyze relationship between numerical features and categorical targets\n",
    "print(\"\\n=== FEATURE RELATIONSHIPS WITH CATEGORICAL VARIABLES ===\")\n",
    "\n",
    "# Subscription level vs numerical features\n",
    "if \"level\" in df.columns and existing_num_cols:\n",
    "    print(\"\\nNumerical features by subscription level:\")\n",
    "\n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Select key numerical features for comparison\n",
    "    key_features = [\"itemInSession\", \"length\", \"sessionId\"]\n",
    "\n",
    "    for i, feature in enumerate(key_features):\n",
    "        if feature in df.columns and i < len(axes):\n",
    "            # Box plot comparison\n",
    "            df_clean = df.dropna(subset=[feature, \"level\"])\n",
    "            if len(df_clean) > 0:\n",
    "                df_clean.boxplot(column=feature, by=\"level\", ax=axes[i])\n",
    "                axes[i].set_title(f\"{feature.title()} by Subscription Level\")\n",
    "                axes[i].set_xlabel(\"Subscription Level\")\n",
    "                axes[i].set_ylabel(feature.title())\n",
    "\n",
    "    # User activity comparison\n",
    "    if \"userId\" in df.columns:\n",
    "        user_level_activity = (\n",
    "            df.groupby([\"userId\", \"level\"]).size().reset_index(name=\"activity_count\")\n",
    "        )\n",
    "        user_level_activity.boxplot(column=\"activity_count\", by=\"level\", ax=axes[3])\n",
    "        axes[3].set_title(\"User Activity by Subscription Level\")\n",
    "        axes[3].set_xlabel(\"Subscription Level\")\n",
    "        axes[3].set_ylabel(\"Activity Count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Statistical comparison\n",
    "    from scipy import stats\n",
    "\n",
    "    print(\"\\nStatistical tests comparing subscription levels:\")\n",
    "    for feature in key_features:\n",
    "        if feature in df.columns:\n",
    "            paid_data = df[df[\"level\"] == \"paid\"][feature].dropna()\n",
    "            free_data = df[df[\"level\"] == \"free\"][feature].dropna()\n",
    "\n",
    "            if len(paid_data) > 0 and len(free_data) > 0:\n",
    "                # Perform t-test\n",
    "                t_stat, p_value = stats.ttest_ind(paid_data, free_data)\n",
    "\n",
    "                print(f\"  {feature}:\")\n",
    "                print(\n",
    "                    f\"    Paid users - Mean: {paid_data.mean():.2f}, Std: {paid_data.std():.2f}\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"    Free users - Mean: {free_data.mean():.2f}, Std: {free_data.std():.2f}\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"    T-test p-value: {p_value:.6f} {'(Significant)' if p_value < 0.05 else '(Not significant)'}\"\n",
    "                )\n",
    "\n",
    "# Feature engineering ideas based on correlations\n",
    "print(\"\\n=== FEATURE ENGINEERING INSIGHTS ===\")\n",
    "print(\"Based on correlation analysis and data patterns:\")\n",
    "print(\"1. Session-based features: itemInSession shows user engagement within sessions\")\n",
    "print(\n",
    "    \"2. Temporal features: Activity patterns vary by hour/day - useful for engagement scoring\"\n",
    ")\n",
    "print(\n",
    "    \"3. User behavior ratios: Events per session, sessions per day could be predictive\"\n",
    ")\n",
    "print(\n",
    "    \"4. Subscription comparison: Statistical differences suggest subscription level is important\"\n",
    ")\n",
    "print(\n",
    "    \"5. Missing value patterns: Song-related missing values occur together - may indicate non-music events\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced churn indicators analysis with visualizations\n",
    "print(\"=== POTENTIAL CHURN INDICATORS ===\\n\")\n",
    "\n",
    "# Search for churn-related events\n",
    "if \"page\" in df.columns:\n",
    "    churn_keywords = [\"cancel\", \"downgrade\", \"unsubscrib\", \"logout\", \"error\"]\n",
    "    churn_events = []\n",
    "\n",
    "    for event in df[\"page\"].unique():\n",
    "        if any(keyword in str(event).lower() for keyword in churn_keywords):\n",
    "            churn_events.append(event)\n",
    "\n",
    "    print(f\"Potential churn-related events found: {churn_events}\")\n",
    "\n",
    "    if churn_events:\n",
    "        churn_data = []\n",
    "        for event in churn_events:\n",
    "            event_df = df[df[\"page\"] == event]\n",
    "            count = event_df.shape[0]\n",
    "            users = event_df[\"userId\"].nunique()\n",
    "            churn_data.append(\n",
    "                {\n",
    "                    \"Event\": event,\n",
    "                    \"Count\": count,\n",
    "                    \"Users\": users,\n",
    "                    \"Avg_per_User\": count / users if users > 0 else 0,\n",
    "                }\n",
    "            )\n",
    "            print(f\"  {event}: {count} events, {users} unique users\")\n",
    "\n",
    "        churn_df = pd.DataFrame(churn_data)\n",
    "\n",
    "        # Visualize churn events\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "        # Churn events count\n",
    "        churn_df.set_index(\"Event\")[\"Count\"].plot(\n",
    "            kind=\"bar\", ax=axes[0, 0], color=\"red\", alpha=0.7\n",
    "        )\n",
    "        axes[0, 0].set_title(\"Churn-Related Events Count\")\n",
    "        axes[0, 0].set_ylabel(\"Number of Events\")\n",
    "        axes[0, 0].tick_params(axis=\"x\", rotation=45)\n",
    "        axes[0, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        # Users affected by churn events\n",
    "        churn_df.set_index(\"Event\")[\"Users\"].plot(\n",
    "            kind=\"bar\", ax=axes[0, 1], color=\"orange\", alpha=0.7\n",
    "        )\n",
    "        axes[0, 1].set_title(\"Users Affected by Churn Events\")\n",
    "        axes[0, 1].set_ylabel(\"Number of Users\")\n",
    "        axes[0, 1].tick_params(axis=\"x\", rotation=45)\n",
    "        axes[0, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        # Average events per user for churn events\n",
    "        churn_df.set_index(\"Event\")[\"Avg_per_User\"].plot(\n",
    "            kind=\"bar\", ax=axes[1, 0], color=\"darkred\", alpha=0.7\n",
    "        )\n",
    "        axes[1, 0].set_title(\"Average Churn Events per User\")\n",
    "        axes[1, 0].set_ylabel(\"Events per User\")\n",
    "        axes[1, 0].tick_params(axis=\"x\", rotation=45)\n",
    "        axes[1, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        # Churn events timeline\n",
    "        if \"datetime\" in df.columns:\n",
    "            churn_timeline = (\n",
    "                df[df[\"page\"].isin(churn_events)]\n",
    "                .groupby([df[\"datetime\"].dt.date, \"page\"])\n",
    "                .size()\n",
    "                .unstack(fill_value=0)\n",
    "            )\n",
    "            churn_timeline.index = pd.to_datetime(churn_timeline.index)\n",
    "\n",
    "            for event in churn_events:\n",
    "                if event in churn_timeline.columns:\n",
    "                    churn_timeline[event].plot(\n",
    "                        ax=axes[1, 1], alpha=0.7, linewidth=2, label=event\n",
    "                    )\n",
    "\n",
    "            axes[1, 1].set_title(\"Churn Events Timeline\")\n",
    "            axes[1, 1].set_xlabel(\"Date\")\n",
    "            axes[1, 1].set_ylabel(\"Daily Churn Events\")\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Enhanced user engagement analysis\n",
    "if \"userId\" in df.columns:\n",
    "    print(\"\\n=== USER ENGAGEMENT ANALYSIS ===\\n\")\n",
    "\n",
    "    # Calculate comprehensive engagement metrics per user\n",
    "    engagement_metrics = (\n",
    "        df.groupby(\"userId\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"page\": [\n",
    "                    \"count\",\n",
    "                    lambda x: x.nunique(),\n",
    "                ],  # Total events and unique event types\n",
    "                \"sessionId\": \"nunique\",  # Unique sessions\n",
    "                \"ts\": lambda x: (x.max() - x.min())\n",
    "                / (1000 * 60 * 60 * 24),  # Activity span in days\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    engagement_metrics.columns = [\n",
    "        \"userId\",\n",
    "        \"total_events\",\n",
    "        \"unique_event_types\",\n",
    "        \"unique_sessions\",\n",
    "        \"activity_span_days\",\n",
    "    ]\n",
    "    engagement_metrics[\"events_per_day\"] = engagement_metrics[\"total_events\"] / (\n",
    "        engagement_metrics[\"activity_span_days\"] + 1\n",
    "    )\n",
    "    engagement_metrics[\"events_per_session\"] = (\n",
    "        engagement_metrics[\"total_events\"] / engagement_metrics[\"unique_sessions\"]\n",
    "    )\n",
    "\n",
    "    print(\"Engagement metrics summary:\")\n",
    "    print(\n",
    "        engagement_metrics[\n",
    "            [\n",
    "                \"total_events\",\n",
    "                \"unique_event_types\",\n",
    "                \"unique_sessions\",\n",
    "                \"activity_span_days\",\n",
    "                \"events_per_day\",\n",
    "            ]\n",
    "        ].describe()\n",
    "    )\n",
    "\n",
    "    # Visualize engagement metrics\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    metrics = [\n",
    "        \"total_events\",\n",
    "        \"unique_event_types\",\n",
    "        \"unique_sessions\",\n",
    "        \"activity_span_days\",\n",
    "        \"events_per_day\",\n",
    "        \"events_per_session\",\n",
    "    ]\n",
    "    colors = [\"blue\", \"green\", \"red\", \"orange\", \"purple\", \"brown\"]\n",
    "\n",
    "    for i, (metric, color) in enumerate(zip(metrics, colors, strict=False)):\n",
    "        engagement_metrics[metric].hist(\n",
    "            bins=30, ax=axes[i], alpha=0.7, color=color, edgecolor=\"black\"\n",
    "        )\n",
    "        axes[i].set_title(f'Distribution of {metric.replace(\"_\", \" \").title()}')\n",
    "        axes[i].set_xlabel(metric.replace(\"_\", \" \").title())\n",
    "        axes[i].set_ylabel(\"Number of Users\")\n",
    "        axes[i].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        # Add mean line\n",
    "        mean_val = engagement_metrics[metric].mean()\n",
    "        axes[i].axvline(\n",
    "            mean_val,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.8,\n",
    "            label=f\"Mean: {mean_val:.1f}\",\n",
    "        )\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # User segmentation based on engagement\n",
    "    print(\"\\n=== USER SEGMENTATION BY ENGAGEMENT ===\")\n",
    "\n",
    "    # Define engagement tiers based on quantiles\n",
    "    engagement_metrics[\"engagement_tier\"] = pd.cut(\n",
    "        engagement_metrics[\"events_per_day\"],\n",
    "        bins=[\n",
    "            0,\n",
    "            engagement_metrics[\"events_per_day\"].quantile(0.25),\n",
    "            engagement_metrics[\"events_per_day\"].quantile(0.5),\n",
    "            engagement_metrics[\"events_per_day\"].quantile(0.75),\n",
    "            engagement_metrics[\"events_per_day\"].max(),\n",
    "        ],\n",
    "        labels=[\"Low\", \"Medium-Low\", \"Medium-High\", \"High\"],\n",
    "        include_lowest=True,\n",
    "    )\n",
    "\n",
    "    tier_summary = engagement_metrics[\"engagement_tier\"].value_counts().sort_index()\n",
    "    print(\"User engagement tiers:\")\n",
    "    print(tier_summary)\n",
    "\n",
    "    # Visualize user tiers\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    tier_summary.plot(\n",
    "        kind=\"pie\", autopct=\"%1.1f%%\", colors=sns.color_palette(\"viridis\", 4)\n",
    "    )\n",
    "    plt.title(\"User Distribution by Engagement Tier\")\n",
    "    plt.ylabel(\"\")\n",
    "\n",
    "    # Box plot of engagement metrics by tier\n",
    "    plt.subplot(2, 2, 2)\n",
    "    engagement_metrics.boxplot(\n",
    "        column=\"events_per_day\", by=\"engagement_tier\", ax=plt.gca()\n",
    "    )\n",
    "    plt.title(\"Events per Day by Engagement Tier\")\n",
    "    plt.xlabel(\"Engagement Tier\")\n",
    "    plt.ylabel(\"Events per Day\")\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    engagement_metrics.boxplot(\n",
    "        column=\"total_events\", by=\"engagement_tier\", ax=plt.gca()\n",
    "    )\n",
    "    plt.title(\"Total Events by Engagement Tier\")\n",
    "    plt.xlabel(\"Engagement Tier\")\n",
    "    plt.ylabel(\"Total Events\")\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    engagement_metrics.boxplot(\n",
    "        column=\"activity_span_days\", by=\"engagement_tier\", ax=plt.gca()\n",
    "    )\n",
    "    plt.title(\"Activity Span by Engagement Tier\")\n",
    "    plt.xlabel(\"Engagement Tier\")\n",
    "    plt.ylabel(\"Activity Span (Days)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Identify potentially churned users (low engagement)\n",
    "    low_engagement_threshold = engagement_metrics[\"events_per_day\"].quantile(0.1)\n",
    "    low_engagement_users = engagement_metrics[\n",
    "        engagement_metrics[\"events_per_day\"] <= low_engagement_threshold\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        f\"\\nUsers with potentially low engagement (bottom 10%): {len(low_engagement_users)}\"\n",
    "    )\n",
    "    print(f\"Low engagement threshold: {low_engagement_threshold:.2f} events per day\")\n",
    "\n",
    "    # Analyze churn events by subscription level\n",
    "    if \"level\" in df.columns and churn_events:\n",
    "        print(\"\\n=== CHURN EVENTS BY SUBSCRIPTION LEVEL ===\")\n",
    "        churn_by_level = (\n",
    "            df[df[\"page\"].isin(churn_events)]\n",
    "            .groupby([\"level\", \"page\"])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "        )\n",
    "        print(\"Churn events by subscription level:\")\n",
    "        print(churn_by_level)\n",
    "\n",
    "        # Visualize churn by level\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        churn_by_level.plot(kind=\"bar\", stacked=True, colormap=\"Set3\")\n",
    "        plt.title(\"Churn Events by Subscription Level\")\n",
    "        plt.xlabel(\"Subscription Level\")\n",
    "        plt.ylabel(\"Number of Events\")\n",
    "        plt.legend(title=\"Churn Event\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== INITIAL DATA EXPLORATION SUMMARY ===\\n\")\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(\n",
    "    f\"   â€¢ Mini dataset: {df.shape[0]:,} events from {df['userId'].nunique() if 'userId' in df.columns else 'N/A'} users\"\n",
    ")\n",
    "print(\n",
    "    f\"   â€¢ Time period: {(df['datetime'].max() - df['datetime'].min()).days if 'datetime' in df.columns else 'N/A'} days\"\n",
    ")\n",
    "print(\n",
    "    f\"   â€¢ Event types: {df['page'].nunique() if 'page' in df.columns else 'N/A'} unique types\"\n",
    ")\n",
    "\n",
    "print(\"\\nData Quality:\")\n",
    "missing_cols = missing_info[missing_info[\"Missing_Percentage\"] > 0]\n",
    "if not missing_cols.empty:\n",
    "    print(f\"   â€¢ Missing values found in {len(missing_cols)} columns\")\n",
    "    for _, row in missing_cols.iterrows():\n",
    "        print(f\"     - {row['Column']}: {row['Missing_Percentage']:.1f}% missing\")\n",
    "else:\n",
    "    print(\"   â€¢ No missing values detected\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(f\"   â€¢ Duplicate records: {duplicates:,} ({duplicates/len(df)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"   â€¢ No duplicate records found\")\n",
    "\n",
    "print(\"\\nUser Behavior Insights:\")\n",
    "if \"page\" in df.columns:\n",
    "    top_events = df[\"page\"].value_counts().head(3)\n",
    "    print(\n",
    "        f\"   â€¢ Top 3 events: {', '.join([f'{event} ({count})' for event, count in top_events.items()])}\"\n",
    "    )\n",
    "\n",
    "if \"level\" in df.columns:\n",
    "    level_split = df[\"level\"].value_counts()\n",
    "    print(f\"   â€¢ Subscription levels: {dict(level_split)}\")\n",
    "\n",
    "if churn_events:\n",
    "    print(f\"   â€¢ Potential churn indicators found: {', '.join(churn_events)}\")\n",
    "else:\n",
    "    print(\"   â€¢ No obvious churn events detected in page types\")\n",
    "\n",
    "print(\"\\nKey Challenges Identified:\")\n",
    "challenges = []\n",
    "if any(missing_info[\"Missing_Percentage\"] > 10):\n",
    "    challenges.append(\"High missing values in some columns\")\n",
    "if duplicates / len(df) > 0.01:\n",
    "    challenges.append(\"Duplicate records present\")\n",
    "if not churn_events:\n",
    "    challenges.append(\"Churn definition needs to be inferred from user behavior\")\n",
    "\n",
    "if challenges:\n",
    "    for i, challenge in enumerate(challenges, 1):\n",
    "        print(f\"   {i}. {challenge}\")\n",
    "else:\n",
    "    print(\"   No major data quality issues identified\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"   1. Define churn criteria based on user inactivity patterns\")\n",
    "print(\"   2. Perform detailed churn analysis and user segmentation\")\n",
    "print(\"   3. Engineer features capturing user engagement trends\")\n",
    "print(\"   4. Address data quality issues before modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Export for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data with additional features for next notebook\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Export summary statistics\n",
    "summary_stats = {\n",
    "    \"total_events\": len(df),\n",
    "    \"unique_users\": df[\"userId\"].nunique() if \"userId\" in df.columns else 0,\n",
    "    \"unique_event_types\": df[\"page\"].nunique() if \"page\" in df.columns else 0,\n",
    "    \"date_range_days\": (\n",
    "        (df[\"datetime\"].max() - df[\"datetime\"].min()).days\n",
    "        if \"datetime\" in df.columns\n",
    "        else 0\n",
    "    ),\n",
    "    \"potential_churn_events\": churn_events,\n",
    "    \"data_quality_issues\": challenges,\n",
    "}\n",
    "\n",
    "print(\"Initial data exploration completed!\")\n",
    "print(\"Summary statistics and processed data ready for churn definition analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
