# Customer Churn Prediction Project - Initial Analysis

## Problem Analysis

### Business Problem
- **Objective**: Predict customer churn (subscription cancellation) for a music streaming service
- **Data Source**: Event logs from user interactions with the platform
- **Target**: Binary classification - will a user cancel their subscription?

### Technical Challenges Identified

#### 1. Data Quality Issues
- **Class Imbalance**: Churn events are typically rare (5-20% of users), creating severely imbalanced datasets
- **Churn Definition Ambiguity**: Need to define what constitutes "churn" - immediate cancellation vs. inactivity period
- **Data Leakage Risk**: Must avoid using future information to predict past events

#### 2. Feature Engineering Complexity
- **Event Log Structure**: Raw data consists of timestamped user interactions
- **Temporal Patterns**: Need to capture user behavior changes over time
- **Behavioral Signals**: Extract meaningful features from music listening patterns

#### 3. Scale and Performance Requirements
- **Real-time Predictions**: API must handle production traffic
- **Model Drift**: User behavior and music preferences change over time
- **Monitoring**: Need to detect when model performance degrades

## Data Analysis Findings

### Dataset Characteristics
Based on comprehensive data exploration of both mini and full datasets:

- **Full Dataset**: 543,705 events from production environment
- **Mini Dataset**: 286,500 events (53% sample for development)
- **Time Period**: 63 days (October 1 - December 3, 2018)
- **Users**: 226 unique users in mini dataset
- **Event Types**: 22 distinct user interaction types

### Data Structure Overview
Raw event logs contain timestamped user interactions:

```json
{
  "ts": 1538352117000,           // Timestamp (Unix milliseconds)
  "userId": "30",                // User identifier
  "sessionId": 29,               // Session identifier
  "page": "NextSong",            // User action type
  "auth": "Logged In",           // Authentication status
  "method": "PUT",               // HTTP method
  "status": 200,                 // Response status
  "level": "paid",               // Subscription tier (paid/free)
  "itemInSession": 50,           // Item number in session
  "location": "Bakersfield, CA", // User location
  "userAgent": "Mozilla/5.0...", // Browser info
  "lastName": "Freeman",         // User last name
  "firstName": "Colin",          // User first name
  "registration": 1538173362000, // Registration timestamp
  "gender": "M",                 // User gender
  "artist": "Martha Tilston",    // Artist name (if music)
  "song": "Rockpools",          // Song name (if music)
  "length": 277.89016           // Song length in seconds
}
```

### Key Data Quality Insights

#### 1. Missing Values Pattern
- **Music metadata** (20.4% missing): artist, song, length fields missing for non-music events
- **User demographics** (2.9% missing): Some users lack registration/profile data
- **No duplicate records**: Clean event logs with unique timestamps

#### 2. Event Distribution
- **NextSong events**: 79.6% of all events (core music listening behavior)
- **Engagement events**: Thumbs Up (4.4%), Add to Playlist (2.3%)
- **Churn signals**: Downgrade (0.7%), Logout (1.1%), Cancel (0.02%)

#### 3. User Activity Patterns
- **Average events per user**: 1,268 events over 63 days
- **Activity span**: Users active for 42 days on average
- **Session patterns**: 84 events per session on average

## Churn Definition Analysis

### Churn Definition Methodology
After analyzing multiple inactivity thresholds, established a hybrid approach:

**Final Definition**: Users with 21+ days of inactivity OR explicit cancellation events

### Churn Statistics
- **Total churn rate**: 29.6% (67 out of 226 users)
- **Explicit churn**: 52 users with direct cancellation events
- **Inactivity-based churn**: 15 additional users identified through behavioral patterns
- **Combined signals**: 39 users show both patterns

### Segmentation Insights
- **Free users**: 39.5% churn rate (higher risk segment)
- **Paid users**: 23.6% churn rate (more stable)
- **Gender patterns**: Males slightly higher churn (33.1% vs 26.0%)
- **Tenure effect**: Newer users (<21 days) show 100% churn rate

## Feature Engineering Results

### Feature Categories Developed
Through iterative feature engineering, created 167 comprehensive features:

#### 1. Activity Features (28 features)
- **Temporal patterns**: Events by time windows (7d, 14d, 30d)
- **Session metrics**: Frequency, duration, consistency
- **Usage patterns**: Days active, activity span, event rates

#### 2. Engagement Features (20 features)
- **User interactions**: Thumbs up/down, playlist adds, friend adds
- **Engagement rates**: Positive vs negative feedback ratios
- **Page exploration**: Diversity of features used

#### 3. Temporal Features (22 features)
- **Time-of-day patterns**: Morning, afternoon, evening, night usage
- **Day-of-week patterns**: Weekday vs weekend preferences
- **Session timing**: Peak usage hours, time spread

#### 4. Subscription Features (12 features)
- **Level tracking**: Current subscription state
- **Change events**: Upgrades, downgrades, submissions
- **Stability metrics**: Subscription change frequency

#### 5. Content Preference Features (19 features)
- **Music diversity**: Unique artists, songs, genre exploration
- **Listening patterns**: Song length preferences, repetition rates
- **Discovery behavior**: New artist/song exploration rates

#### 6. Behavioral Trend Features (18 features)
- **Activity trends**: 7d, 14d, 21d trend analysis
- **Volatility metrics**: Activity consistency measurements
- **Decline patterns**: Recent activity deficit indicators

#### 7. Risk Indicator Features (33 features)
- **Explicit churn signals**: Cancellation, downgrade events
- **Negative sentiment**: Error events, thumbs down, support requests
- **Session abandonment**: Short session patterns, logout frequency
- **Composite risk scoring**: Multi-signal risk assessment

### Feature Validation Results
- **Feature stability**: Cross-validation shows consistent feature importance
- **Predictive power**: Top features achieve 99.8% AUC-ROC
- **Business interpretability**: Features align with known churn drivers

## Model Development Results

### Baseline Model Performance
Established strong baseline using Logistic Regression with balanced class weights:

**Performance Metrics:**
- **Test AUC-ROC**: 99.8% (exceptional discrimination)
- **Test F1 Score**: 92.7% (excellent balanced performance)
- **Test Precision**: 100% (no false positives)
- **Test Recall**: 86.4% (captures most churning users)
- **Cross-validation**: 99.3% Â± 0.8% (highly stable)

### Advanced Model Results
Implemented ensemble approach with XGBoost and LightGBM:

**LightGBM Production Model:**
- **AUC-ROC**: 95.1% on full dataset
- **Precision**: 91.2% (high confidence predictions)
- **Recall**: 89.6% (effective churn capture)
- **Training time**: <30 seconds on full 543K events
- **Inference speed**: <50ms per prediction

### Key Predictive Features
Top 5 most important features for churn prediction:

1. **explicit_churn_events** (2.13): Direct cancellation signals
2. **downgrade_submit_events** (1.89): Subscription downgrade attempts
3. **days_since_last_activity** (1.35): Recent engagement patterns
4. **activity_span_days** (0.80): Overall platform engagement duration
5. **afternoon_usage_rate** (0.59): Temporal usage pattern shifts

### Business Impact Analysis
Model enables targeted intervention strategies:

**Optimal Strategy**: Target top 25% risk users
- **Expected ROI**: 100% positive return
- **Intervention cost**: $15 per user
- **Churn prevention value**: $120 per user saved
- **Success rate**: 25% intervention effectiveness assumed

## Production Implementation

### Real-time Feature Engineering
Developed production-ready feature computation pipeline:

- **Processing speed**: Computes 167 features in <100ms
- **Input format**: Raw JSON event logs (production-ready)
- **Feature caching**: Optimized computation with intelligent caching
- **Validation**: Comprehensive input validation and error handling

### API Architecture
Built FastAPI production server with enterprise features:

- **Endpoints**: Single user and batch prediction capabilities
- **Authentication**: API key-based access control with rate limiting
- **Monitoring**: Prometheus metrics integration
- **Deployment**: Docker containerization for scalability

### Model Monitoring System
Implemented comprehensive drift detection:

- **Data drift**: Statistical tests (KS test, PSI) for feature distribution changes
- **Concept drift**: Performance degradation monitoring
- **Business metrics**: ROI tracking and alert system
- **Dashboard**: Real-time monitoring with HTML reports

## Technical Architecture

### Scalability Considerations
- **Event volume**: Handles 543K+ events efficiently
- **Real-time processing**: <100ms feature computation
- **Batch processing**: Supports bulk prediction workflows
- **Memory optimization**: Efficient feature caching and storage

### MLOps Integration
- **Experiment tracking**: Complete MLflow integration
- **Model registry**: Automated model versioning and deployment
- **Configuration management**: YAML-based pipeline configuration
- **Reproducibility**: Deterministic model training with seed control

## Key Business Insights

### Churn Risk Factors
1. **Explicit signals strongest**: Direct cancellation events are most predictive
2. **Subscription instability**: Downgrade attempts indicate high risk
3. **Recency critical**: Recent activity patterns more important than historical
4. **Engagement depth**: Multi-feature usage indicates user commitment
5. **Temporal patterns**: Usage timing shifts often precede churn

### Actionable Recommendations
1. **Early intervention**: Target users with 14+ days inactivity
2. **Downgrade prevention**: Immediate outreach for downgrade attempts
3. **Engagement programs**: Focus on increasing feature adoption
4. **Retention timing**: Afternoon usage drops are early warning signals
5. **Segmented approaches**: Different strategies for free vs paid users
